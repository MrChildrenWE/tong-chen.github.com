---
title: Records for bayes learning
author: Chen Tong
layout: post
description: 贝叶斯学习记录
modified:
categories:
  - math
tags:
  - bayes
---

这篇文章用于记录学习贝叶斯定理及其应用过程中的记录，希望由浅及深的提供
一份自我学习教程。

### 引子

* 概率的定义：概率是一个0-1之间的数，代表了我们对某个事实或预测的相信程度。

* 条件概率: 指基于某种背景信息的概率值。

* 联合概率：指2个或多个事件同时发生的概率。

* 事件独立性：一个事件的发生不影响其他事件即为事件的独行性。

* 概率的数学表示：事件A发生的概率写作 P(A), 事件B发生的概率写作 P(B), 
  给定事件A后事件B发生的概率 P(B|A), 事件A和B同时发生的概率P(A and B)。

* 事件的独立性用数学公式表示为：P(B|A) = P(B), P(A|B) = P(A) 
  A事件是否发生对B事件的发生没有影响，反之亦然，
  即表明A事件与B事件独立。

* 联合概率：P(A and B) = P(A) * P(B) 当事件A和B独立时，即
  P(B|A) = P(B), P(A|B) = P(A) 时。

  若事件A和事件B不一定相互独立呢？更通用的法则是：

  ```
  P(A and B) = P(A) * P(B|A)
  P(A and B) = P(B) * P(A|B)
  ```

* 我们举个例子：假设有袋圆球，罐1中有30个黑球和10个白球，罐2中黑球和白球各20个。某人随机的从一个罐子中取出一粒球，发现是黑球，问这个黑球从罐1中取出的概率有多大？

  这个问题怎么解答呢？

  问题是：黑球从罐1中取出的概率多大；这句话包含了2个事件，`黑球`和`罐1`.

  假如我们知道取得黑球的概率P(黑球)和给定黑球后球是从罐1取得的概率
  P(罐1|黑球)(这个是我们要计算的，假设个变量标记下就好), 
  我们可以计算出联合概率：
  P(黑球 and 罐1) = P(黑球)*P(罐1|黑球)

  另外我们也可以先选择罐1，然后再取出黑球，这样联合概率就是：	
  P(黑球 and 罐1) = P(罐1)*P(黑球|罐1)

  综合以上2个公式，我们就可以得到：
  
  ```
  P(黑球)*P(罐1|黑球) = P(罐1)*P(黑球|罐1)

  P(罐1|黑球) = P(罐1)*P(黑球|罐1) / P(黑球)
	              
  = P(罐1)*P(黑球|罐1) / (P(罐1)*P(黑球|罐1)+P(罐2)*P(黑球|罐2))

  = 0.5 * 0.75 / (0.5 * 0.75 + 0.5 * 0.5)

  = 0.6	
  ```

  注：这是一个简单的例子作为引子，是一个非常规解法。
  例子中的P(黑球)可以比较容易计算，所
  以我们只需要一步就可以算出黑球从罐1中取出的概率有多大。

### 贝叶斯定理

* 基于联合概率和条件概率的贝叶斯定理推导

  对于任意两个事件A和B，P(A and B) = P(B and A);

  ```
  P(A and B) = P(A) * P(B|A)

  P(B and A) = P(B) * P(A|B)

  P(A|B) = P(A)*P(B|A)/P(B)

  P(B|A) = P(B)*P(A|B)/P(A)
  ```

  在有了这两个转换之后，我们就可以用已知的或者容易观察的数据来计算未知
  的，不容易观察到的部分。

* 贝叶斯定理解释: 贝叶斯定理反应的是随着数据的更新而得以矫正的概率值。

  设定H代表我们的假说，D代表观测数据，贝叶斯定理可以写做

  ```
  P(H|D) = P(H)*P(D|H) / P(D)
  ```  
  
  * P(H): 先验概率，反应的是主观对假说H的认可度，
		  反应的是获得观察数据之前的认识。

  * P(H|D): 后验概率，在分析了观察数据之后对假说H的新的认识，
			反应的是根据新的事实对H发生的概率的更新。

  * P(D|H): 似然值，在假设成立的条件下，可以获得这组观察数据的概率。

  * P(D): 在任何假设条件下，获取到这组观察数据的概率。通常难以计算。

  * 在这个公式中，观察数据是已经获得的。假说也是容易提出的。
    在假说成立的条件下，数据的模式是可以估计的。
	三个变量，这儿解决了2个。

  * 通常情况下，为了规避对P(D)的计算，我们会穷举出所有独立的假设 
    (在这些假设中，最多有一个是真的，也至少有一个是真的)，
	分别计算P(H1|D), P(H2|D), P(H3|D)..., 
	根据所有这些概率的和为1进行归一化，获得各个假设在给定的当前数据模
	式下成立的概率。

  * About normalizing constant, we simplify things by specifying a set
    of hypotheses that are mutually exclusive and collectively
	exhaustive.
  
  * 在遇到问题时，D和H也并不总会很清晰，既需要我们多梳理问题，
    明确哪个观察数据是有意义的，更需要我们熟悉较多的例子，
	加深对贝叶斯定理应用的理解。

### 贝叶斯定理应用案例

#### M&M豆问题

公司在不同年份生产的M&M豆包含的不同颜色的豆的比例不同，
1994年产的M&M豆包装中，棕色30%，黄色20%，红色20%，绿色10%，橙色10%，
茶色10%；1996年产的M&M豆包装中，棕色13%，黄色14%，红色13%，绿色20%，
橙色16%，蓝色24%。假设手中有两粒M&M豆，分别是橙色和绿色，
一个来自1994年包装，一个来自1996年包装，求算橙色来源于1994年包装的概率？


解题思路：

  * 观察到的数据 D：橙色球和绿色球个来自不同包装
  
  * 完全穷举独立假设
    
    * 假设A：橙色来源于94，绿色来源于96
	* 假设B：橙色来源于96，绿色来源于94

  * 假设A和假设B发生的概率是一样的，都为0.5.

  * 似然值的计算
  	
    * 假设A：P(橙色|94)*P(绿色|96) = 0.1 * 0.2 = 0.02
	* 假设B：P(橙色|96)*P(绿色|94) = 0.16 * 0.1 = 0.016

	为了计算方便，似然值可以乘以任意一个因子，不影响结果。

    * 假设A：P(橙色|94)*P(绿色|96) = 0.1  * 0.2 * 100 = 20
	* 假设B：P(橙色|96)*P(绿色|94) = 0.16 * 0.1 * 100 = 16

	为了计算方便，似然值可以乘以任意一个因子，不影响结果。

  * 后验概率
  
    * P(A|D) = 0.05 * 20 = 10
	* P(B|D) = 0.05 * 16 = 8

  * Normalize概率：P(A|D) = 10/18 = 5/9。因为穷举了所有假设，
    所以后验概率之和为1.

表格表示

|   |Prior   |Likelihood  |              | Posterior    |
|---|-------:|-----------:|-------------:|-------------:|
|   |P(H)    |P(D|H)      | P(H)*P(D|H)  | P(H|D)       |
|A  |0.5     |10 * 20     |100           |5/9           |
|B  |0.5     |16 * 10     |80            |4/9           |

* 在提出假设时, 假设要完整穷尽，然后给每个假设指定一个代号便于描述和理清思路。

* 不明确的变量和概率值也用一个符号表示，便于列出公式。这一点我们后面还会提到。


#### Monty Hall问题






![MSA and adaptor clipping]({{ site.img_url }}/smRNA-seq-1.jpg)

### References

* [Think bayes](https://github.com/AllenDowney/ThinkBayes)
* [数学之美番外篇：平凡而又神奇的贝叶斯方法](http://mindhacks.cn/2008/09/21/the-magical-bayesian-method/)

